{"options":[{"enabled":false,"definition":{"id":"5d58cc01-7c75-450c-be18-a388ddb129ec"},"inputs":{"branchFilters":"[\"+refs/heads/*\"]","additionalFields":"{}"}},{"enabled":false,"definition":{"id":"a9db38f9-9fdc-478c-b0f9-464221e58316"},"inputs":{"workItemType":"Bug","assignToRequestor":"true","additionalFields":"{}"}}],"triggers":[{"branchFilters":["+master"],"forks":{"enabled":false,"allowSecrets":false},"pathFilters":["+/databricks","+/models"],"requireCommentsForNonTeamMembersOnly":true,"isCommentRequiredForPullRequest":true,"triggerType":64},{"branchFilters":["+master"],"pathFilters":["+/databricks","+/models"],"batchChanges":false,"maxConcurrentBuildsPerBranch":1,"pollingInterval":0,"triggerType":2}],"variables":{"DATABRICKS_CLUSTERID":{"value":"0924-164345-sofa329"},"system.debug":{"value":"false","allowOverride":true}},"variableGroups":[{"variables":{"acr-name":{"value":null,"isSecret":true},"databricks-prod-token":{"value":null,"isSecret":true},"databricks-token":{"value":null,"isSecret":true},"kcmunnin-adls-clientsecret":{"value":null,"isSecret":true}},"type":"AzureKeyVault","name":"Dev Environment","id":1}],"retentionRules":[{"branches":["+refs/heads/*"],"artifacts":[],"artifactTypesToDelete":["FilePath","SymbolStore"],"daysToKeep":10,"minimumToKeep":1,"deleteBuildRecord":true,"deleteTestResults":true}],"properties":{},"tags":[],"_links":{"self":{"href":"https://dev.azure.com/glrcsu/b02cb8ef-cd70-463d-a505-17989df23d4c/_apis/build/Definitions/18?revision=66"},"web":{"href":"https://dev.azure.com/glrcsu/b02cb8ef-cd70-463d-a505-17989df23d4c/_build/definition?definitionId=18"},"editor":{"href":"https://dev.azure.com/glrcsu/b02cb8ef-cd70-463d-a505-17989df23d4c/_build/designer?id=18&_a=edit-build-definition"},"badge":{"href":"https://dev.azure.com/glrcsu/b02cb8ef-cd70-463d-a505-17989df23d4c/_apis/build/status/18"}},"jobAuthorizationScope":1,"jobTimeoutInMinutes":60,"jobCancelTimeoutInMinutes":5,"process":{"phases":[{"steps":[{"environment":{},"enabled":true,"continueOnError":false,"alwaysRun":false,"displayName":"Use Python 3.6","timeoutInMinutes":0,"condition":"succeeded()","task":{"id":"33c63b11-352b-45a2-ba1b-54cb568a29ca","versionSpec":"0.*","definitionType":"task"},"inputs":{"versionSpec":"3.6","addToPath":"true","architecture":"x64"}},{"environment":{},"enabled":true,"continueOnError":false,"alwaysRun":false,"displayName":"Bash - Install Pre Reqs","timeoutInMinutes":0,"condition":"succeeded()","task":{"id":"6c731c3c-3c68-459a-a5c9-bde6e6595b5b","versionSpec":"3.*","definitionType":"task"},"inputs":{"targetType":"inline","filePath":"","arguments":"","script":"# KC Munnings, 2019\n# Sample Code.  Not for production use.\n\n# Install pre-reqs such as Databricks CLI, JQ, etc.\nsudo apt-get install build-essential -y\nsudo pip3 install --upgrade pip\nsudo pip3 install --upgrade pip setuptools wheel\nsudo pip3 install databricks-cli\nsudo apt-get install jq","workingDirectory":"","failOnStderr":"false","noProfile":"true","noRc":"true"}},{"environment":{},"enabled":true,"continueOnError":false,"alwaysRun":false,"displayName":"Bash - Test Job Configuration","timeoutInMinutes":0,"condition":"succeeded()","refName":"","task":{"id":"6c731c3c-3c68-459a-a5c9-bde6e6595b5b","versionSpec":"3.*","definitionType":"task"},"inputs":{"targetType":"inline","filePath":"","arguments":"","script":"# KC Munnings, 2019\n# Sample Code.  Not for production use.\n\n##########\n# Job Config\n##########\n\necho \"Raw Template\"\ncat $(Build.Repository.LocalPath)/databricks/job-template.json\n\n# Tokenize Job Config File with clusterid and repo name\n# The cluster ID is stored in a variable instead of key vault variable group because key vault is encrypted and can't be written to a plaintext file\necho -e \"Cluster ID: $(DATABRICKS_CLUSTERID)\"\n\nsed \\\n-e 's@#{repo-name}#@$(Build.Repository.Name)@' \\\n-e 's@#{cluster-id}#@$(DATABRICKS_CLUSTERID)@' \\\n-e 's@#{master-notebook}#@/Shared/build/$(Build.Repository.Name)/test-master.r@' \\\n$(Build.Repository.LocalPath)/databricks/job-template.json>job-$(Build.BuildId).json\n\n# Echo Job Config to Agent\necho -e \"\\nTokenized Template\"\ncat job-$(Build.BuildId).json\n\n##########\n# Databricks CLI Config\n##########\n\n# Have to include this in path so \"databricks\" command works\n# PATH=$PATH:/home/kcmunninadmin/.local/bin\n\n#Point to correct Databricks environment\nexport DATABRICKS_HOST=https://eastus2.azuredatabricks.net\nexport DATABRICKS_TOKEN=$(databricks-token)\n\n# Ensure Repo Build Directory Exists\ndatabricks workspace mkdirs /Shared/build/$(Build.Repository.Name)\n\n##########\n# Job Creation\n##########\n\n# Define source and target for \"master notebook\" which runs tests with parameters for all builds\nexport BUILD_SOURCE_PATH=$(Build.Repository.LocalPath)/databricks/test-master.r\nexport DATABRICKS_TARGET_WS_PATH=/Shared/build/$(Build.Repository.Name)/test-master.r\n\n#Copy Test Master File\ndatabricks workspace import $BUILD_SOURCE_PATH $DATABRICKS_TARGET_WS_PATH -o -l R\n\n# Make test file hidden so it is not included in build folder in the databricks mounted storage\nmv $BUILD_SOURCE_PATH $(Build.Repository.LocalPath)/databricks/.test-master.r\n\n# Check if Job exists\nJOBID=$(databricks jobs list --output json | jq '.jobs[] | select(.settings.name==\"build-$(Build.Repository.Name)\") | .job_id')\n# Write Databricks JOBID to Pipeline Variable\necho \"##vso[task.setvariable variable=JOBID]$JOBID\"\n\n# Create job if not exists\nif [ -z \"$JOBID\" ]\nthen\n    echo -e \"Creating Databricks Job: build-$(Build.Repository.Name)\"\n    JOBID=$(databricks jobs create --json-file ./job-$(Build.BuildId).json | jq '.job_id')\n    echo \"##vso[task.setvariable variable=JOBID]$JOBID\"\n    echo -e \"job id: $JOBID\"\nelse\n    echo -e \"Job ID $JOBID already exists for build-$(Build.Repository.Name)\"\nfi","workingDirectory":"","failOnStderr":"false","noProfile":"true","noRc":"true"}},{"environment":{},"enabled":true,"continueOnError":false,"alwaysRun":false,"displayName":"Bash - Copy Notebooks, Models and Tests","timeoutInMinutes":0,"condition":"succeeded()","task":{"id":"6c731c3c-3c68-459a-a5c9-bde6e6595b5b","versionSpec":"3.*","definitionType":"task"},"inputs":{"targetType":"inline","filePath":"","arguments":"","script":"# Microsoft, 2019\n# Sample Code.  Not for production use.\n\n# Have to include this in path so \"databricks\" command works\n# PATH=$PATH:/home/kcmunninadmin/.local/bin\n\n#Point to correct Databricks environment\nexport DATABRICKS_HOST=https://eastus2.azuredatabricks.net\nexport DATABRICKS_TOKEN=$(databricks-token)\n\n# Deploy Databricks workspace artifacts to workspace\nexport DATABRICKS_TARGET_WS_DIR=/Shared/build/$(Build.Repository.Name)/$(Build.BuildId)\nexport BUILD_SOURCE_DIR=$(Build.Repository.LocalPath)/databricks/\ndatabricks workspace import_dir $BUILD_SOURCE_DIR $DATABRICKS_TARGET_WS_DIR -o -e\n\n# Deploy model to mounted storage\nexport MODEL_SOURCE=$(Build.Repository.LocalPath)/models/\nexport MODEL_TARGET=dbfs:/mnt/adlsv2-databricks/build/$(Build.Repository.Name)/$(Build.BuildId)/models/\ndatabricks fs cp $MODEL_SOURCE $MODEL_TARGET -r\n\n# Deploy tests to mounted storage\nexport TEST_SOURCE=$(Build.Repository.LocalPath)/databricks/test/\nexport TEST_TARGET=dbfs:/mnt/adlsv2-databricks/build/$(Build.Repository.Name)/$(Build.BuildId)/test/\ndatabricks fs cp $TEST_SOURCE $TEST_TARGET -r\n\n# Deploy small sample data for tests to mounted storage\nexport DATA_SOURCE=$(Build.Repository.LocalPath)/data/\nexport DATA_TARGET=dbfs:/mnt/adlsv2-databricks/build/$(Build.Repository.Name)/$(Build.BuildId)/data/\ndatabricks fs cp $DATA_SOURCE $DATA_TARGET -r","workingDirectory":"","failOnStderr":"false","noProfile":"false","noRc":"false"}},{"environment":{},"enabled":true,"continueOnError":false,"alwaysRun":false,"displayName":"Bash - Run Test Job","timeoutInMinutes":0,"condition":"succeeded()","task":{"id":"6c731c3c-3c68-459a-a5c9-bde6e6595b5b","versionSpec":"3.*","definitionType":"task"},"inputs":{"targetType":"inline","filePath":"","arguments":"","script":"# Have to include this in path so \"databricks\" command works\nPATH=$PATH:/home/kcmunninadmin/.local/bin\n\n#Point to correct Databricks environment\nexport DATABRICKS_HOST=https://eastus2.azuredatabricks.net\nexport DATABRICKS_TOKEN=$(databricks-token)\n\n# Create testing result directory in mounted storage and copy tests over\ndatabricks fs mkdirs dbfs:/mnt/adlsv2-databricks/build/$(Build.Repository.Name)/$(Build.BuildId)/test-result\n\n\n# Run Job - Tests\necho -e \"Job id:  $(JOBID)\"\nJOB_PARAMS=$( jq -n -c \\\n                  --arg build_number \"$(Build.BuildId)\" \\\n                  --arg repo_name \"$(Build.Repository.Name)\" \\\n                  --arg notebook_path \"/Shared/build/$(Build.Repository.Name)/$(Build.BuildId)/test/run-tests\" \\\n                  --arg data_test_path \"/dbfs/mnt/adlsv2-databricks/build/$(Build.Repository.Name)/$(Build.BuildId)/data/test/r_input_file1.csv\" \\\n                  --arg data_train_path \"/dbfs/mnt/adlsv2-databricks/build/$(Build.Repository.Name)/$(Build.BuildId)/data/train/weight_data.csv\" \\\n                  '{build_number: $build_number, repo_name: $repo_name, notebook_path: $notebook_path, data_test_path: $data_test_path, data_train_path: $data_train_path}' )\necho -e \"Job Parameters: $JOB_PARAMS\"\n\nRUNID=$(databricks jobs run-now --job-id $(JOBID) --notebook-params $JOB_PARAMS | jq '.run_id')\necho -e \"run id: $RUNID\"\n\n# Poll Job Run for status\nTIME_PASSED=0\nTIMEOUT=60\nJOB_STATUS=\"\"\n\nwhile [ $TIME_PASSED -le $TIMEOUT ]\ndo\n    # Get Job Status\n    JOB_STATUS=$(databricks runs get --run-id $RUNID | jq '.state.life_cycle_state' -r)\n    echo \"Polling Job ID: $(JOBID), Run ID: $RUNID, Status: $JOB_STATUS\"\n    if [ \"$JOB_STATUS\" = \"TERMINATED\" ]\n    then\n        JOB_STATUS=$(databricks runs get --run-id $RUNID | jq '.state.result_state' -r)\n        if [ \"$JOB_STATUS\" = \"SUCCESS\" ]\n        then\n            echo \"Polling Job ID: $(JOBID), Run ID: $RUNID, Status: $JOB_STATUS\"\n            # Copy test result files to agent\n            export TEST_RESULT_SOURCE=dbfs:/mnt/adlsv2-databricks/build/$(Build.Repository.Name)/$(Build.BuildId)/test-result/\n            export TEST_RESULT_TARGET=$(Build.Repository.LocalPath)/databricks/test-result/\n            echo \"Copying test-result files from $TEST_RESULT_SOURCE to $TEST_RESULT_TARGET\"\n            databricks fs cp $TEST_RESULT_SOURCE $TEST_RESULT_TARGET -r --overwrite\n            break\n        fi\n        if [ \"$JOB_STATUS\" = \"FAILED\" ]\n        then\n            # Fail the build\n            # Future Code Here\n            echo \"failed\"\n            break\n        fi\n    fi\n    sleep 10\n    TIME_PASSED=$(($TIME_PASSED + 10))\ndone","workingDirectory":"","failOnStderr":"false","noProfile":"true","noRc":"true"}},{"environment":{},"enabled":true,"continueOnError":false,"alwaysRun":false,"displayName":"Test Results - Publish","timeoutInMinutes":0,"condition":"succeeded()","task":{"id":"0b0f01ed-7dde-43ff-9cbb-e48954daf9b1","versionSpec":"2.*","definitionType":"task"},"inputs":{"testRunner":"JUnit","testResultsFiles":"$(Build.Repository.LocalPath)/databricks/test-result/*.xml","searchFolder":"$(System.DefaultWorkingDirectory)","mergeTestResults":"false","failTaskOnFailedTests":"true","testRunTitle":"","platform":"","configuration":"","publishRunAttachments":"true"}},{"environment":{},"enabled":false,"continueOnError":false,"alwaysRun":false,"displayName":"Copy - Artifact Files","timeoutInMinutes":0,"condition":"and(succeeded(), eq(variables['Build.SourceBranchName'], 'master'))","task":{"id":"5bfb729a-a7c8-4a78-a7c3-8d717bb7c13c","versionSpec":"2.*","definitionType":"task"},"inputs":{"SourceFolder":"","Contents":"models/**\ndatabricks/score/**","TargetFolder":"$(Build.ArtifactStagingDirectory)","CleanTargetFolder":"false","OverWrite":"false","flattenFolders":"false","preserveTimestamp":"false"}},{"environment":{},"enabled":false,"continueOnError":false,"alwaysRun":false,"displayName":"Bash - Echo Artifact Contents","timeoutInMinutes":0,"condition":"and(succeeded(), eq(variables['Build.SourceBranchName'], 'master'))","task":{"id":"6c731c3c-3c68-459a-a5c9-bde6e6595b5b","versionSpec":"3.*","definitionType":"task"},"inputs":{"targetType":"inline","filePath":"","arguments":"","script":"# Echo Stage contents\nls -ls -R $(Build.ArtifactStagingDirectory)","workingDirectory":"","failOnStderr":"false","noProfile":"true","noRc":"true"}},{"environment":{},"enabled":false,"continueOnError":false,"alwaysRun":false,"displayName":"Publish Artifact: Notebooks","timeoutInMinutes":0,"condition":"and(succeeded(), eq(variables['Build.SourceBranchName'], 'master'))","task":{"id":"2ff763a7-ce83-4e1f-bc89-0ae63477cebe","versionSpec":"1.*","definitionType":"task"},"inputs":{"PathtoPublish":"$(Build.ArtifactStagingDirectory)","ArtifactName":"databricks-files","ArtifactType":"Container","TargetPath":"","Parallel":"false","ParallelCount":"8","FileCopyOptions":""}},{"environment":{},"enabled":true,"continueOnError":false,"alwaysRun":false,"displayName":"Bash - Cleanup Workspace and Mounted Storage","timeoutInMinutes":0,"condition":"succeeded()","task":{"id":"6c731c3c-3c68-459a-a5c9-bde6e6595b5b","versionSpec":"3.*","definitionType":"task"},"inputs":{"targetType":"inline","filePath":"","arguments":"","script":"# Have to include this in path so \"databricks\" command works\nPATH=$PATH:/home/kcmunninadmin/.local/bin\n\n#Point to correct Databricks environment\nexport DATABRICKS_HOST=https://eastus2.azuredatabricks.net\nexport DATABRICKS_TOKEN=$(databricks-token)\n\n# Clean Workspace\ndatabricks workspace rm /Shared/build/$(Build.Repository.Name)/$(Build.BuildId) -r --debug\n\n# Clean Mounted Storage\ndatabricks fs rm dbfs:/mnt/adlsv2-databricks/build/$(Build.Repository.Name)/$(Build.BuildId) -r --debug","workingDirectory":"","failOnStderr":"false","noProfile":"true","noRc":"true"}}],"name":"Configure","refName":"Job_1","condition":"succeeded()","target":{"executionOptions":{"type":0},"allowScriptsAuthAccessOption":false,"type":1},"jobAuthorizationScope":1}],"type":1},"repository":{"properties":{"apiUrl":"https://api.github.com/repos/kcm117/azure-rmodel-devops","branchesUrl":"https://api.github.com/repos/kcm117/azure-rmodel-devops/branches","cloneUrl":"https://github.com/kcm117/azure-rmodel-devops.git","connectedServiceId":"25d6e0f6-30ba-418e-828f-58037b28376c","defaultBranch":"master","fullName":"kcm117/azure-rmodel-devops","hasAdminPermissions":"False","isFork":"False","isPrivate":"False","lastUpdated":"10/01/2019 21:00:16","manageUrl":"https://github.com/kcm117/azure-rmodel-devops","nodeId":"MDEwOlJlcG9zaXRvcnkxNzY4Mzk5NjA=","ownerId":"11845433","orgName":"kcm117","refsUrl":"https://api.github.com/repos/kcm117/azure-rmodel-devops/git/refs","safeRepository":"kcm117/azure-rmodel-devops","shortName":"azure-rmodel-devops","ownerAvatarUrl":"https://avatars2.githubusercontent.com/u/11845433?v=4","archived":"False","externalId":"176839960","ownerIsAUser":"True","checkoutNestedSubmodules":"false","cleanOptions":"0","fetchDepth":"0","gitLfsSupport":"false","reportBuildStatus":"true","skipSyncSource":"false","labelSourcesFormat":"$(build.buildNumber)","labelSources":"0"},"id":"kcm117/azure-rmodel-devops","type":"GitHub","name":"kcm117/azure-rmodel-devops","url":"https://github.com/kcm117/azure-rmodel-devops.git","defaultBranch":"master","clean":"false","checkoutSubmodules":false},"processParameters":{},"quality":1,"authoredBy":{"displayName":"KC Munnings","url":"https://spsprodcus2.vssps.visualstudio.com/A4b9f2c77-595f-450c-b159-33f537c42fe4/_apis/Identities/4e8b8cf2-c549-4b01-a87b-319763c52ab3","_links":{"avatar":{"href":"https://dev.azure.com/glrcsu/_apis/GraphProfile/MemberAvatars/aad.ZjdlNDU5Y2QtMzc1MS03YjUyLTljY2MtNmY0NWY0ODE2MTdi"}},"id":"4e8b8cf2-c549-4b01-a87b-319763c52ab3","uniqueName":"kcmunnin@microsoft.com","imageUrl":"https://dev.azure.com/glrcsu/_apis/GraphProfile/MemberAvatars/aad.ZjdlNDU5Y2QtMzc1MS03YjUyLTljY2MtNmY0NWY0ODE2MTdi","descriptor":"aad.ZjdlNDU5Y2QtMzc1MS03YjUyLTljY2MtNmY0NWY0ODE2MTdi"},"drafts":[],"queue":{"_links":{"self":{"href":"https://dev.azure.com/glrcsu/_apis/build/Queues/46"}},"id":46,"name":"Hosted Ubuntu 1604","url":"https://dev.azure.com/glrcsu/_apis/build/Queues/46","pool":{"id":6,"name":"Hosted Ubuntu 1604","isHosted":true}},"id":18,"name":"ci-rmodel-databricks","url":"https://dev.azure.com/glrcsu/b02cb8ef-cd70-463d-a505-17989df23d4c/_apis/build/Definitions/18?revision=66","uri":"vstfs:///Build/Definition/18","path":"\\r-models","type":2,"queueStatus":0,"revision":66,"createdDate":"2019-10-01T21:14:33.420Z","project":{"id":"b02cb8ef-cd70-463d-a505-17989df23d4c","name":"ds-devops","description":"Sample devops project for data science, using R","url":"https://dev.azure.com/glrcsu/_apis/projects/b02cb8ef-cd70-463d-a505-17989df23d4c","state":1,"revision":183,"visibility":0,"lastUpdateTime":"2019-06-13T12:36:46.923Z"}}